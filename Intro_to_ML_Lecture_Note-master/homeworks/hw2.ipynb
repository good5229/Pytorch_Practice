{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI-UA 0473 - Introduction to Machine Learning\n",
    "## HW 2 - Programming Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Recovering Input (10 points)\n",
    "For this programming assignment, an image (vector) was chosen and was passed through a weird function to obtain something mysterious (vector). \n",
    "\n",
    "You are given the mysterious vector and the weird function used to transform the original image. \n",
    "\n",
    "AND, YOUR TASK IS TO FIND THE ORIGINAL IMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from autograd import numpy\n",
    "from autograd import grad\n",
    "\n",
    "from scipy import optimize\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mysterious vector\n",
    "\n",
    "import pickle    # Library used to save and load values of variables (Technically, it's called Serialization)\n",
    "\n",
    "shp_original_img = (100, 100)  # Shape of the original image\n",
    "mysterious_image = pickle.load(open('mysterious_image_t.p', 'rb'), encoding='latin1') # mysterious_image is a vector representing the morphed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The weird function which transformed the original image\n",
    "# x: a 1D array with 10000 elements.\n",
    "def weird_function(x, n_iter=5):\n",
    "    h = x    \n",
    "    filt = numpy.array([-1./3, 1./3, -1./3])\n",
    "    \n",
    "    for ii in range(n_iter):\n",
    "        h_l = numpy.concatenate([numpy.array([0]), h[:-1]])\n",
    "        h_r = numpy.concatenate([h[1:], numpy.array([0])])\n",
    "        h = filt[0] * h + filt[-1] * h_l + filt[1] * h_r\n",
    "        \n",
    "        if numpy.mod(ii, 2) == 0:\n",
    "            h = numpy.concatenate([h[h.shape[0]//2:],h[:h.shape[0]//2]])\n",
    "            \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Please write your implementation to find the original image and store it in the `original_image` variable.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hints**: <ul>\n",
    "    <li> Can you reformulate this as an optimization problem?  If so, <ul>\n",
    "<li> You will find `autograd` helpful. </li>\n",
    "<li> Also, if you used `scipy.optimize.minimize()` in your solution and found your program slow, try passing the argument `method='CG'`. </li>\n",
    "        </ul> </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "original_image = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This call will show something reasonable if your code is correct.\n",
    "plot.imshow(original_image.reshape(shp_original_img), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 - Numerical Stability (10 points)\n",
    "\n",
    "In Lab 3 we found that the logistic regression implementation is not numerically stable.  In particular, the gradient computation would yield NaN values given inputs with a large magnitude.\n",
    "\n",
    "In this assignment, your goal is to re-implement logistic regression in a numerically stable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Numerical stability analysis of negative log probability\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Beware**: Although you don't have to write anything for this section, please read it to figure out where numerical stability issues would occur.  Such analysis is likely required for almost all scientific computation code if you write your own, regardless of platforms and programming languages.\n",
    "</div>\n",
    "\n",
    "Recall that the empirical cost function for logistic regression is an average of per-sample loss functions, each of which is the negative log probability of the correct label for a given sample:\n",
    "\n",
    "$$\n",
    "D_\\mathrm{lr}(y_i, M, \\mathbf{x}_i) = -y_i \\log \\mathrm{sigmoid}(\\mathbf{w}^T \\mathbf{x} + b) - (1 - y_i) \\log (1 - \\mathrm{sigmoid}(\\mathbf{w}^T \\mathbf{x} + b))\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathrm{sigmoid}(x) = \\dfrac{1}{1 + \\exp (-x)}\n",
    "$$\n",
    "\n",
    "We can see that a naive implementation would have two potential stability issues:\n",
    "1. If $\\mathrm{sigmoid}(\\mathbf{w}^T \\mathbf{x} + b))$ is either close to 0 or 1, then one of the logarithm terms in the first equation will give us $-\\infty$.\n",
    "2. If $x$ is negative with a large enough absolute value, then $\\exp(-x)$ in the second equation will become $+\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Rewriting the negative log probability\n",
    "\n",
    "After the identification of stability issues, the first thing we usually try is to rewrite the entire formulation in order to avoid most of the issues *while* maintaining precision as much as possible.\n",
    "\n",
    "Suppose that we have a numerically stable implementation of a *log-sigmoid* function.  It is possible to express the per-sample loss function (hence the empirical cost function itself)\n",
    "* Using `log_sigmoid()`, and\n",
    "* Without using either `numpy.log()` or `numpy.exp()` outside `log_sigmoid()`.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Please fill in (1) the implementation of a numerically stable `log_sigmoid()` function, and (2) the implementation of `logreg_cost` to compute the *empirical cost function* of logistic regression, using `log_sigmoid()` but without using `numpy.log()` and `numpy.exp()` elsewhere.  You don't have to give the mathematical derivation for your implementation.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "**Hint**: The derivation of the more numerically stable negative log probability for a *single* sample goes as follows:\n",
    "<ol>\n",
    "    <li> Denote $s = \\mathbf{w}^T \\mathbf{x} + b$. </li>\n",
    "    <li> Rewrite the per example loss with $\\log \\mathrm{sigmoid}(s)$ as a whole term, without any other logarithms or exponentials. </li>\n",
    "    <li> Now take a look at $\\log \\mathrm{sigmoid}(s) = \\log \\frac{1}{1 + \\exp(-s)}$.  We don't have any problem computing this quantity with a positive $s$, but how do we avoid overflow when $s$ is negative with a large magnitude?  (Hint: re-express this quantity using $\\log \\mathrm{sigmoid}(-s)$).  How do we combine both cases together in `numpy`? </li>\n",
    "    <li> For positive $s$, rewrite $\\log \\mathrm{sigmoid}(s)$ to eliminate the fraction inside the logarithm.  Use `numpy.log1p()` to compute the quantity $\\log (1 + x)$ since it avoids underflow with small $x$.\n",
    "</ol><br>\n",
    "The procedure above for computing a more precise $\\log \\mathrm{sigmoid}(x)$ is a special case of the more general *log-sum-exp* trick, which is the basis of the numerical stability for multiclass logistic regression (which will be covered later in the course).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sigmoid_naive(s):\n",
    "    # DO NOT CHANGE\n",
    "    return numpy.log(1 / (1 + numpy.exp(-s)))\n",
    "\n",
    "def log_one_minus_sigmoid_naive(s):\n",
    "    # DO NOT CHANGE\n",
    "    return numpy.log(1 - 1 / (1 + numpy.exp(-s)))\n",
    "\n",
    "def logreg_cost_naive(params, x, y):\n",
    "    # DO NOT CHANGE\n",
    "    w, b = params[:-1], params[-1]\n",
    "    s = x @ w + b\n",
    "    return numpy.mean(-y * log_sigmoid_naive(s) - (1 - y) * log_one_minus_sigmoid_naive(s))\n",
    "\n",
    "def log_sigmoid(s):\n",
    "    # YOUR CODE HERE: rewrite this function in a numerically stable way.\n",
    "    return log_sigmoid_naive(s)\n",
    "\n",
    "def logreg_cost(params, x, y):\n",
    "    # YOUR CODE HERE: rewrite the cost function.\n",
    "    # y: a 1D array of 0-1 labels with shape (n_samples,)\n",
    "    # x: a 2D array (matrix) with shape (n_samples, n_features).  Each row contains an input vector.\n",
    "    # params: a 1D array of logistic regression parameters, with shape (n_samples + 1,)\n",
    "    return logreg_cost_naive(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell consists of two tests which ensures that\n",
    "* your solution is close to the naive solution for \"normal\" numbers, and\n",
    "* your solution would not throw NaNs in either cost or gradient computation for large numbers.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "**Beware**: Passing the two tests does **NOT** necessarily mean that your code is correct.  For instance, you can easily pass these tests with a solution that calls the naive functions for \"normal\" values, and always return 0 for large values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Tests for correctness\n",
    "# DO NOT CHANGE\n",
    "\n",
    "logreg_rule = grad(logreg_cost)\n",
    "logreg_rule_naive = grad(logreg_cost_naive)\n",
    "n_features = 10\n",
    "n_samples = 100\n",
    "\n",
    "def test(scale, normal):\n",
    "    # Throws an error if the test fails\n",
    "    center_positive = numpy.ones(n_features) * scale\n",
    "    center_negative = -numpy.ones(n_features) * scale\n",
    "    std = scale\n",
    "    for _ in range(10):\n",
    "        # Since we don't really care about classification itself, we just randomly generate a bunch of params, x's and y's.\n",
    "        params = numpy.random.randn(n_features + 1)\n",
    "        params[-1] = 0\n",
    "        x, y = make_blobs(n_samples=n_samples,\n",
    "                          n_features=n_features,\n",
    "                          centers=[center_positive, center_negative], cluster_std=std, shuffle=True)\n",
    "\n",
    "        cost = logreg_cost(params, x, y)\n",
    "        dcost_dparams = logreg_rule(params, x, y)\n",
    "        \n",
    "        if normal:\n",
    "            cost_naive = logreg_cost_naive(params, x, y)\n",
    "            dcost_dparams_naive = logreg_rule_naive(params, x, y)\n",
    "            # Throws an error if the two arrays differ significantly\n",
    "            assert numpy.allclose(cost, cost_naive)\n",
    "            assert numpy.allclose(dcost_dparams, dcost_dparams_naive)\n",
    "        else:\n",
    "            assert not numpy.isnan(cost) and not numpy.isinf(cost)\n",
    "            assert not numpy.any(numpy.isnan(dcost_dparams) | numpy.isinf(dcost_dparams))\n",
    "    \n",
    "# Precision test: your solution should be close to the naive solution for \"normal\" numbers.\n",
    "test(1, True)\n",
    "# Stability test: your solution should not throw NaN in either the cost or gradient computation for large numbers.\n",
    "test(10000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
